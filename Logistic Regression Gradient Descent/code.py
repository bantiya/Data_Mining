# -*- coding: utf-8 -*-
"""lasthopeass2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nWWSZmQZjjfDXTYPcrpOtuFp7-GOSOZa
"""

from google.colab import drive

drive.mount('/content/gdrive')

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
df = pd.read_csv('/content/gdrive/MyDrive/sem 2/584/Assignment/Assignment 2/cleveland-train.csv')
df

df.rename(columns={'heartdisease::category|-1|1': 'target'}, inplace=True)
df

df.isna().sum()

ax = sns.countplot(x="target", hue="sex", data=df)
plt.legend(labels=["Female","Male"])

a = pd.get_dummies(df['cp'], prefix = "cp")
b = pd.get_dummies(df['thal'], prefix = "thal")
c = pd.get_dummies(df['slope'], prefix = "slope")
frames = [df, a, b, c]
df = pd.concat(frames, axis = 1)

df = df.drop(columns = ['cp', 'thal', 'slope'])
df

class LogisticRegresssion:

  def __init__(self, lr = 0.00001, n_iters=10000, tol = 10^-3):
    self.lr = lr
    self.n_iters = n_iters
    self.weights = None
    self.bias = None
    self.tol = tol
  
  def fit(self, X,y):
    n_samples, n_features = X.shape
    self.weights = np.zeros(n_features)
    self.bias = 0

    cost_list = []

    for _ in range(self.n_iters):
      linear_model = np.dot(X, self.weights) + self.bias
      y_predicted = self._sigmoid(linear_model)

      cost = (1/n_samples) * np.sum(np.log(1 + np.exp(-y * np.dot(self.weights, X.T))))

      dw = (1 / n_samples) * np.dot(X.T,(y_predicted - y))
      db = (1 / n_samples) * np.sum(y_predicted - y)

      self.weights -= self.lr * dw
      self.bias -= self.lr * db

      if self.weights.any() < self.tol:
        return cost_list

      cost_list.append(cost)
    return cost_list

  def predict(self,X):
    linear_model = np.dot(X, self.weights) + self.bias
    y_predicted = self._sigmoid(linear_model)
    y_predicted_cls = [1 if i>0.5 else -1 for i in y_predicted]
    return y_predicted_cls

  def _sigmoid(self,x):
    return 1 /(1+ np.exp(-x))

def accuracy(y_true,y_pred):
  accuracy = np.sum(y_true == y_pred) / len(y_true)
  return accuracy

# Importing StandardScaler from scikit-learn
from sklearn.preprocessing import StandardScaler
sst = StandardScaler()

# Standardizing the data apart from the Class column
df_scaled = pd.DataFrame(sst.fit_transform(df.drop('target',axis=1)))

# Adding the Class column back to the DataFrame
df_scaled['Target'] = df.target
df_scaled

X = df_scaled.iloc[:, :-1].values
y = df_scaled.iloc[:, -1].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1234)

print("Shape of X_train : ", X_train.shape)
print("Shape of Y_train : ", y_train.shape)
print("Shape of X_test : ", X_test.shape)
print("Shape of Y_test : ", y_test.shape)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# lr = LogisticRegresssion(0.00001, 10000, 10^-3)
# 
# cost_list = lr.fit(X_train, y_train)
# 
# plt.plot(np.arange(10000), cost_list)
# plt.show()
# 
# prediction = lr.predict(X_test)
# a = accuracy(y_test, prediction)
# 
# print("LR classification accuracy from custom Logistic Regression function:", a)
# print("Classification error", 1 - a)

acc_list = []
acc_list.append(a)
acc_list.append(1-a)

# DataFrame Accuracy 
acc_df = pd.DataFrame()
acc_df['Label']= ['Accuracy(10000)', 'Classification error(10000)']
acc_df['Values']= acc_list
acc_df

# Commented out IPython magic to ensure Python compatibility.
# %%time
# lr = LogisticRegresssion(0.00001, 100000, 10^-3)
# 
# cost_list = lr.fit(X_train, y_train)
# 
# plt.plot(np.arange(100000), cost_list)
# plt.show()
# 
# prediction = lr.predict(X_test)
# b21 = accuracy(y_test, prediction)
# 
# print("LR classification accuracy from custom Logistic Regression function:", b21)
# print("Classification error", 1 - b21)

acc_df = acc_df.append({'Label' : 'Accuracy(100000)', 'Values' : b21}, ignore_index=True)
acc_df = acc_df.append({'Label' : 'Classification error(100000)', 'Values' : 1-b21}, ignore_index=True)
acc_df

# Commented out IPython magic to ensure Python compatibility.
# %%time
# lr = LogisticRegresssion(0.00001, 1000000, 10^-3)
# 
# cost_list = lr.fit(X_train, y_train)
# 
# plt.plot(np.arange(1000000), cost_list)
# plt.show()
# 
# prediction = lr.predict(X_test)
# a12 = accuracy(y_test, prediction)
# 
# print("LR classification accuracy from custom Logistic Regression function:", a12)
# print("Classification error", 1 - a12)

acc_df = acc_df.append({'Label' : 'Accuracy(1000000)', 'Values' : a12}, ignore_index=True)
acc_df = acc_df.append({'Label' : 'Classification error(1000000)', 'Values' : 1-a12}, ignore_index=True)
acc_df

# Commented out IPython magic to ensure Python compatibility.
# %%time
# lr = LogisticRegresssion(0.00001, 10000, 10^-6)
# 
# cost_list = lr.fit(X_train, y_train)
# 
# plt.plot(np.arange(10000), cost_list)
# plt.show()
# 
# prediction = lr.predict(X_test)
# a121 = accuracy(y_test, prediction)
# 
# print("LR classification accuracy from custom Logistic Regression function:", a121)
# print("Classification error", 1 - a121)
# 
# acc_df = acc_df.append({'Label' : 'Accuracy(10000,10^-6)', 'Values' : a121}, ignore_index=True)
# acc_df = acc_df.append({'Label' : 'Classification error(10000,10^-6)', 'Values' : 1-a121}, ignore_index=True)
# acc_df

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# lr = LogisticRegresssion(0.00001, 100000, 10^-6)
# 
# cost_list = lr.fit(X_train, y_train)
# 
# plt.plot(np.arange(100000), cost_list)
# plt.show()
# 
# prediction = lr.predict(X_test)
# ab = accuracy(y_test, prediction)
# 
# print("LR classification accuracy from custom Logistic Regression function:", ab)
# print("Classification error", 1 - ab)
# 
# acc_df = acc_df.append({'Label' : 'Accuracy(100000,10^-6)', 'Values' : ab}, ignore_index=True)
# acc_df = acc_df.append({'Label' : 'Classification error(100000,10^-6)', 'Values' : 1-ab}, ignore_index=True)
# acc_df

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# lr = LogisticRegresssion(0.00001, 1000000, 10^-6)
# 
# cost_list = lr.fit(X_train, y_train)
# 
# plt.plot(np.arange(1000000), cost_list)
# plt.show()
# 
# prediction = lr.predict(X_test)
# ab1 = accuracy(y_test, prediction)
# 
# print("LR classification accuracy from custom Logistic Regression function:", ab1)
# print("Classification error", 1 - ab1)
# 
# acc_df = acc_df.append({'Label' : 'Accuracy(1000000,10^-6)', 'Values' : ab1}, ignore_index=True)
# acc_df = acc_df.append({'Label' : 'Classification error(1000000,10^-6)', 'Values' : 1-ab1}, ignore_index=True)
# acc_df

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from sklearn.linear_model import *
# log = LogisticRegression(random_state=0)
# log.fit(X_train, y_train)
# lr_prediction = log.predict(X_test)
# print("LR classification accuracy from sklearn:", accuracy(y_test, lr_prediction))

acc_df = acc_df.append({'Label' : 'Accuracy(LR sklearn)', 'Values' : accuracy(y_test, lr_prediction)}, ignore_index=True)
acc_df

dft = pd.read_csv('/content/gdrive/MyDrive/sem 2/584/Assignment/Assignment 2/cleveland-train.csv')
dft

dft.rename(columns={'heartdisease::category|-1|1': 'target'}, inplace=True)
a = pd.get_dummies(dft['cp'], prefix = "cp")
b = pd.get_dummies(dft['thal'], prefix = "thal")
c = pd.get_dummies(dft['slope'], prefix = "slope")
frames = [dft, a, b, c]
dft = pd.concat(frames, axis = 1)
dft = dft.drop(columns = ['cp', 'thal', 'slope'])

# Importing StandardScaler from scikit-learn
sst = StandardScaler()

# Standardizing the data apart from the Class column
dft_scaled = pd.DataFrame(sst.fit_transform(dft.drop('target',axis=1)))

# Adding the Class column back to the DataFrame
dft_scaled['target'] = dft.target
dft_scaled

X1 = dft_scaled.iloc[:, :-1].values
y1 = dft_scaled.iloc[:, -1].values
print(X1)

dfts = pd.read_csv('/content/gdrive/MyDrive/sem 2/584/Assignment/Assignment 2/cleveland-test.csv')
dfts

a = pd.get_dummies(dfts['cp'], prefix = "cp")
b = pd.get_dummies(dfts['thal'], prefix = "thal")
c = pd.get_dummies(dfts['slope'], prefix = "slope")
frames = [dfts, a, b, c]
dfts = pd.concat(frames, axis = 1)
dfts = dfts.drop(columns = ['cp', 'thal', 'slope'])

# Importing StandardScaler from scikit-learn
sst = StandardScaler()

# Standardizing the data apart from the Class column
dfts_scaled = pd.DataFrame(sst.fit_transform(dfts))

X2 = dfts_scaled.to_numpy()

lr1 = LogisticRegresssion(0.00001, 100000)

c1 = lr1.fit(X1, y1)

prediction101 = lr1.predict(X2)

f = open("bhavya_p_584.dat", "x")

f.close()

f = open("bhavya_p_584.dat", "a")
for i in prediction101:
  f.write(str(i))
  f.write("\n")
f.close()

f = open("bhavya_p_584.dat", "r")
for x in f:
  print(x)

f.close()