# -*- coding: utf-8 -*-
"""FinalAss1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qeqCA-dWnm8a-Sc4e_JOTNqK1o-4DBXN
"""

import numpy as np
import pandas as pd
import nltk
import sklearn

"""**Mounting Google drive to goggle collab**"""

from google.colab import drive 
drive.mount('/content/gdrive')

"""**Copy the path of Train data file**"""

data = pd.read_table('/content/gdrive/MyDrive/sem 2/584/Assignment 1/train_file.dat',header=None)
data

data.columns = ['Sentiment', 'Review']

"""**Checking if Null Item exist or not**"""

data.isna().sum()

data.dropna(inplace = True)
data.isna().sum()
data.shape

"""**Removing Special Characters from the dataframe**"""

data['Review1'] = data['Review'].fillna('').astype(str).str.replace(r'[^A-Za-z ]', '', regex=True).replace('', np.nan, regex=False)
del data['Review']
data

"""**Converting all the data to lower case**"""

data["R"] = data["Review1"].str.lower()
data

pip install contractions==0.0.18

"""**Expanding common English contractions into text**

"""

import contractions
data['C'] = data['R'].map(lambda x: [contractions.fix(word) for word in x.split()])
data.head()

data['C1'] = [' '.join(map(str, l)) for l in data['C']]
data

del data['R']
del data['C']
data

"""**Removing all the Stopwords from the data**"""

del data['Review1']

nltk.download('stopwords')
from nltk.corpus import stopwords
stop = stopwords.words('english')
data['C2'] = data['C1'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))
data

del data['C1']
data

nltk.download('punkt')
data['T'] = data.apply(lambda data: nltk.word_tokenize(data['C2']), axis=1)
data

del data['C2']
data

nltk.download('averaged_perceptron_tagger')
data['F1'] = data['T'].apply(nltk.tag.pos_tag)
data.head()

del data['T']
data.head()

nltk.download('wordnet')
from nltk.corpus import wordnet
def get_wordnet_pos(tag):
    if tag.startswith('J'):
        return wordnet.ADJ
    elif tag.startswith('V'):
        return wordnet.VERB
    elif tag.startswith('N'):
        return wordnet.NOUN
    elif tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN
        
data['L'] = data['F1'].apply(lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x])
data.head()

del data['F1']
data.head()

from nltk.stem.wordnet import WordNetLemmatizer
wnl = WordNetLemmatizer()
data['L1'] = data['L'].apply(lambda x: [wnl.lemmatize(word, tag) for word, tag in x])
del data['L']
data.head()

data['lem_words'] = [' '.join(map(str, l)) for l in data['L1']]
del data['L1']
data

data.columns = ['Sentiment', 'lem_words']
data

X_train = data['lem_words']
y_train = data['Sentiment']

print(X_train.shape)
print(y_train.shape)

"""**loading LogisticRegression to lr**"""

from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.model_selection import cross_val_score, cross_val_predict


lr = LogisticRegression(random_state=1)

"""**Using CountVectorizer to calculate the accuracy**"""

from sklearn.feature_extraction.text import CountVectorizer
cvec = CountVectorizer()

# fit the training data on the model
cvec.fit(X_train)

#transform training data into sparse matrix
X_train_cvec = cvec.transform(X_train)

# cross val score to predit 
cvec_score = cross_val_score(lr, X_train_cvec, y_train, cv=3 )

"""**Using Tfidf Vectorizer to calculate the accuracy**"""

from sklearn.feature_extraction.text import TfidfVectorizer
tvec = TfidfVectorizer()

# fit the training data on the model
tvec.fit(X_train)

#transform training data into sparse matrix
X_train_tvec = tvec.transform(X_train)

# cross val score to predit 
tvec_score = cross_val_score(lr, X_train_tvec, y_train, cv=3)

"""**Making a dataframe to store all the accuracy values**"""

acc_list = []
acc_list.append(cvec_score.mean())
acc_list.append(tvec_score.mean())

# DataFrame Accuracy 
acc_df = pd.DataFrame()
acc_df['params']= ['cvec', 'tvec']
acc_df['scores']= acc_list
acc_df

"""**Function of CountVectorizer**"""

def count_vec_ngram(para, X_train, y_train):
    cvec_p = CountVectorizer(ngram_range=(para)) 

    cvec_p.fit(X_train)
    X_train_cvec_p = cvec_p.transform(X_train)

    # cross val score/ predict
    cvec_score_p = cross_val_score(lr, X_train_cvec_p, y_train, cv=3)

    # cross validation 
    return cvec_score_p.mean()

temp1 = count_vec_ngram((1,1), X_train, y_train)
temp2 = count_vec_ngram((1,2), X_train, y_train)
temp3 = count_vec_ngram((1,3), X_train, y_train)
temp4 = count_vec_ngram((1,4), X_train, y_train)

acc_df = acc_df.append({'params' : 'cvec_ngram1', 'scores' : temp1}, ignore_index=True)
acc_df = acc_df.append({'params' : 'cvec_ngram2', 'scores' : temp2}, ignore_index=True)
acc_df = acc_df.append({'params' : 'cvec_ngram3', 'scores' : temp3}, ignore_index=True)
acc_df = acc_df.append({'params' : 'cvec_ngram4', 'scores' : temp4}, ignore_index=True)
acc_df

"""**Function of TfidfVectorizer**"""

def tf_vec_ngram(params, X_train, y_train):
    tfvec_p = TfidfVectorizer(ngram_range=(params)) 

    tfvec_p.fit(X_train)
    X_train_tfvec_p = tfvec_p.transform(X_train)

    # cross val score/ predict
    tfvec_score_p = cross_val_score(lr, X_train_tfvec_p, y_train, cv=3)

    # cross validation 
    return tfvec_score_p.mean()

temp11 = tf_vec_ngram((1,1), X_train, y_train)
temp22 = tf_vec_ngram((1,2), X_train, y_train)
temp33 = tf_vec_ngram((1,3), X_train, y_train)
temp44 = tf_vec_ngram((1,4), X_train, y_train)

acc_df = acc_df.append({'params' : 'tfvec_ngram1', 'scores' : temp11}, ignore_index=True)
acc_df = acc_df.append({'params' : 'tfvec_ngram2', 'scores' : temp22}, ignore_index=True)
acc_df = acc_df.append({'params' : 'tfvec_ngram3', 'scores' : temp33}, ignore_index=True)
acc_df = acc_df.append({'params' : 'tfvec_ngram4', 'scores' : temp44}, ignore_index=True)
acc_df

tdata = pd.read_table('/content/gdrive/MyDrive/sem 2/584/Assignment 1/train_file.dat',header=None)
tdata

tdata.columns = ['Sentiment', 'Review']
tdata

tdata.isna().sum()

tdata.shape

tdata.dropna(inplace = True)

tdata.isna().sum()
tdata.shape

X_train = data['lem_words']
y_train = data['Sentiment']

print(X_train.shape)
print(y_train.shape)

temp121 = count_vec_ngram((1,2), X_train, y_train)
acc_df = acc_df.append({'params' : 'unprocessed_cvec_ngram2', 'scores' : temp121}, ignore_index=True)
acc_df

test_data="/content/gdrive/MyDrive/sem 2/584/Assignment 1/test.dat"

f = open(test_data,'r')
test_data=f.readlines()
f.close()
print(len(test_data))

test_data=pd.DataFrame(test_data)
test_data

test_data.columns = ['Review']
test_data

test_data.isna().sum()

from sklearn.feature_extraction.text import CountVectorizer
vec = CountVectorizer(ngram_range=(1,4))
X_train1 = vec.fit_transform(X_train)
X_test1 = vec.transform(test_data['Review'])

from sklearn.linear_model import *
log = LogisticRegression(random_state=0)
log.fit(X_train1, y_train)

lr_prediction = log.predict(X_test1)

print(lr_prediction)

f = open("bhavya_p_584.dat", "x")

f.close()

f = open("bhavya_p_584.dat", "a")
for i in lr_prediction:
  f.write(str(i))
  f.write("\n")
f.close()

f = open("bhavya_p_584.dat", "r")
for x in f:
  print(x)

f.close()





































































































































